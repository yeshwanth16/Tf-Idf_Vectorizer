{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "\n",
    "\n",
    "# Fit function: To display the word with its dimension pair in the corpus.\n",
    "def fit(dataset):\n",
    "    unique_words = set()\n",
    "    if isinstance(dataset, (list,)):\n",
    "        for row in dataset:\n",
    "            for word in row.split(\" \"):\n",
    "                if len(word)<2:#and word not in unique_words:\n",
    "                    continue\n",
    "                unique_words.add(word) # Add each unique word of length>2 to the list\n",
    "        unique_words=sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)} # Enumerate the list, i.e., give consecutive numbers to each item, store in a dict\n",
    "        l=len(dataset)+1  # total number of documents in corpus\n",
    "        idf=[]   # to store idf value of every word \n",
    "        w=0       # to find number of docs. containing the word  \n",
    "        for i in list(vocab):    #T iterate every word in vocab calculated\n",
    "            w=1 \n",
    "            for row in dataset:    # iterate each document to find the word\n",
    "                if i in row.split():\n",
    "                    w=w+1     # increase the count when word is found\n",
    "            idf1=1+math.log(l/w)  # IDF formula: 1+(log(total no. of docs.(N) /1+no. of docs. containing the word)\n",
    "            idf.append(idf1)\n",
    "        vocab_dict={i:j for i,j in zip(list(vocab),idf)}  #Storing word and idf in a dictonary \n",
    "    return vocab,vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1.916290731874155, 'document': 1.2231435513142097, 'first': 1.5108256237659907, 'is': 1.0, 'one': 1.916290731874155, 'second': 1.916290731874155, 'the': 1.0, 'third': 1.916290731874155, 'this': 1.0}\n"
     ]
    }
   ],
   "source": [
    "vocab,vocab_dict=fit(corpus)  \n",
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform function to find Tf-Idf matrix\n",
    "def transform(dataset,vocab):\n",
    "    rows = []\n",
    "    columns =[]\n",
    "    values = []\n",
    "    for idx, row in enumerate(tqdm(dataset)):  #iterate through each document in the corpus\n",
    "        word_freq = dict(Counter(row.split())) \n",
    "        for word, freq in word_freq.items():\n",
    "            rows.append(idx) #to store index of the doc.\n",
    "            columns.append(vocab.get(word)) #to store dimensions of word                  \n",
    "            #TF=no. of times word occurs/total words in a document\n",
    "            #IDF=1+(log(total no. of docs.(N) /1+no. of docs. containing the word)\n",
    "            tfidf=(freq/len(row.split()))*vocab_dict.get(word)#TF*IDF\n",
    "            values.append(tfidf)\n",
    "    tfid= csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))\n",
    "    tfid=normalize(tfid)\n",
    "    return tfid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "tfidfvec=transform(corpus, vocab)\n",
    "print(tfidfvec.shape)\n",
    "print(list(vocab.keys()))\n",
    "print(tfidfvec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing results with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "vec= TfidfVectorizer()\n",
    "vec.fit(corpus)\n",
    "feature_matrix_2 = vec.transform(corpus)\n",
    "print(feature_matrix_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
